{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports, and Checking PyTorch Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DQN, ReplayBuffer, greedy_action, epsilon_greedy, update_target, loss\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1: DQN Tuning**\n",
    "At each point in the following segment, we vary one specific hyperparameter, keeping the others fixed. As a consequence of this, the other hyperparameters are typically not the ideal one *before* that hyperparameter's been tested. This doesn't significanly matter, however, as all fixed hyperparameters are chosen reasonably so that the model can still provide representative results of how varying hyperparameters affect return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter 1:** Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ These are the hyperparameters which we change in the DQN\n",
    "architecture = [[4, 4, 2], [4, 8, 2], [4, 16, 2], [4, 32, 2], [4, 64, 2],\n",
    "                [4, 4, 4, 2], [4, 8, 4, 2], [4, 16, 8, 2], [4, 32, 16, 2], [4, 64, 32, 2]]\n",
    "buffer_size = 50000\n",
    "batch_size = 128\n",
    "update_frequency = 1\n",
    "learning_rate = 0.001\n",
    "NUM_RUNS = 10\n",
    "EPSILON = 1\n",
    "epsilon_decay = 0.9\n",
    "min_epsilon = 0.1\n",
    "architecture_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store results per run\n",
    "runs_results = []\n",
    "# Make CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "# For each architecture in the architectures\n",
    "for arch_runs in range(len(architecture)):\n",
    "    # Print which architecture we're currently testing\n",
    "    print('Architecture: ', architecture[arch_runs])\n",
    "    runs_results = []\n",
    "    # For each run given in NUM_RUNS\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "        # Create the main and target networks\n",
    "        policy_net = DQN(architecture[arch_runs])\n",
    "        target_net = DQN(architecture[arch_runs])\n",
    "        # Update the target with the main\n",
    "        update_target(target_net, policy_net)\n",
    "        # Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "        target_net.eval()\n",
    "        \n",
    "        # Create an Adam optimizer on the policy network parameters, with the chosen learning rate\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "        # Create a memory buffer of size buffer_size\n",
    "        memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Number of steps done\n",
    "        steps_done = 0\n",
    "        # Storing all the episodes\n",
    "        episode_durations = []\n",
    "\n",
    "        # For each episode in episode_range, defined as 300 in the problem statement\n",
    "        for i_episode in range(300):\n",
    "            # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "            if (i_episode+1) % 50 == 0:\n",
    "                print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "            # Reset the environment, obtain an observation\n",
    "            observation, info = env.reset()\n",
    "            # Turn the observation into a state\n",
    "            state = torch.tensor(observation).float()\n",
    "\n",
    "            # This defines whether we've finished this episode or not, where end is either when we've reached\n",
    "            # 500, or when the pole drops\n",
    "            done = False\n",
    "            terminated = False\n",
    "            t = 0\n",
    "            # While not done with episode\n",
    "            while not (done or terminated):\n",
    "\n",
    "                # Pick the epsilon greedy action from the policy network\n",
    "                action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "                # Take a step in the direction of the action\n",
    "                observation, reward, done, terminated, info = env.step(action)\n",
    "                # Get the reward and action as tensors\n",
    "                reward = torch.tensor([reward])\n",
    "                action = torch.tensor([action])\n",
    "                # Define the next state as the new observation but in tensor format\n",
    "                next_state = torch.tensor(observation).reshape(-1).float()\n",
    "                # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "                memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network), if we've reached batch size\n",
    "                if not len(memory.buffer) < batch_size:\n",
    "                    # Get a random sample from the buffer of size batch_size\n",
    "                    transitions = memory.sample(batch_size)\n",
    "                    # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "                    # sampled transition\n",
    "                    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                    # Compute the MSE loss between the target and policy\n",
    "                    mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    mse_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # If not done as in haven't reached end or terminated, append timestep\n",
    "                if done or terminated:\n",
    "                    episode_durations.append(t + 1)\n",
    "                # Increment the timestep\n",
    "                t = t + 1\n",
    "\n",
    "                # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION - Epsilon here decays to 0.1\n",
    "                if EPSILON > min_epsilon:\n",
    "                    EPSILON = EPSILON * epsilon_decay\n",
    "                else:\n",
    "                    EPSILON = min_epsilon\n",
    "\n",
    "            # Update the target network, copying all weights and biases in DQN only at the update_frequency defined\n",
    "            if i_episode % update_frequency == 0: \n",
    "                # Update the target\n",
    "                update_target(target_net, policy_net)\n",
    "        # Append episodes to results of run\n",
    "        runs_results.append(episode_durations)\n",
    "    # Append runs of specific architecture to that architecture's runs\n",
    "    architecture_runs.append(runs_results)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# SAVE ARCHITECTURE RUNS TO LOAD EASIER\n",
    "# import pickle\n",
    "# with open ('architecture_1Update.pkl', 'wb') as f:\n",
    "#     pickle.dump(architecture_runs, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# PLOT ARCHITECTURE RUNS\n",
    "# Get the architecture run array as a tensor, find mean and STD for each\n",
    "results0 = torch.tensor(architecture_runs[0])\n",
    "means0 = results0.float().mean(0)\n",
    "stds0 = results0.float().std(0)\n",
    "\n",
    "results1 = torch.tensor(architecture_runs[1])\n",
    "means1 = results1.float().mean(0)\n",
    "stds1 = results1.float().std(0)\n",
    "\n",
    "results2 = torch.tensor(architecture_runs[2])\n",
    "means2 = results2.float().mean(0)\n",
    "stds2 = results2.float().std(0)\n",
    "\n",
    "results3 = torch.tensor(architecture_runs[3])\n",
    "means3 = results3.float().mean(0)\n",
    "stds3 = results3.float().std(0)\n",
    "\n",
    "results4 = torch.tensor(architecture_runs[4])\n",
    "means4 = results4.float().mean(0)\n",
    "stds4 = results4.float().std(0)\n",
    "\n",
    "results5 = torch.tensor(architecture_runs[5])\n",
    "means5 = results5.float().mean(0)\n",
    "stds5 = results5.float().std(0)\n",
    "\n",
    "results6 = torch.tensor(architecture_runs[6])\n",
    "means6 = results6.float().mean(0)\n",
    "stds6 = results6.float().std(0)\n",
    "\n",
    "results7 = torch.tensor(architecture_runs[7])\n",
    "means7 = results7.float().mean(0)\n",
    "stds7 = results7.float().std(0)\n",
    "\n",
    "results8 = torch.tensor(architecture_runs[8])\n",
    "means8 = results8.float().mean(0)\n",
    "stds8 = results8.float().std(0)\n",
    "\n",
    "results9 = torch.tensor(architecture_runs[9])\n",
    "means9 = results9.float().mean(0)\n",
    "stds9 = results9.float().std(0)\n",
    "\n",
    "# Plot each architecture mean\n",
    "plt.plot(torch.arange(300), means0, label=\"Architecture = [4, 4, 2]\")\n",
    "plt.plot(torch.arange(300), means1, label=\"Architecture = [4, 8, 2]\")\n",
    "plt.plot(torch.arange(300), means2, label=\"Architecture = [4, 16 2]\")\n",
    "plt.plot(torch.arange(300), means3, label=\"Architecture = [4, 32, 2]\")\n",
    "plt.plot(torch.arange(300), means4, label=\"Architecture = [4, 64, 2]\")\n",
    "plt.plot(torch.arange(300), means5, label=\"Architecture = [4, 4, 4, 2]\")\n",
    "plt.plot(torch.arange(300), means6, label=\"Architecture = [4, 8, 4, 2]\")\n",
    "plt.plot(torch.arange(300), means7, label=\"Architecture = [4, 16, 8, 2]\")\n",
    "plt.plot(torch.arange(300), means8, label=\"Architecture = [4, 32, 16, 2]\")\n",
    "plt.plot(torch.arange(300), means9, label=\"Architecture = [4, 64, 32, 2]\")\n",
    "# Plot each architecture STD\n",
    "plt.fill_between(np.arange(300), means0-stds0, means0+stds0, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means1-stds1, means1+stds1, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means2-stds2, means2+stds2, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means3-stds3, means3+stds3, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means4-stds4, means4+stds4, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means5-stds5, means5+stds5, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means6-stds6, means6+stds6, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means7-stds7, means7+stds7, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means8-stds8, means8+stds8, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means9-stds9, means9+stds9, alpha=0.2, color='dimgray')\n",
    "\n",
    "# Define plot parameters - x label, y label, the 100 horizontal line, etc\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Network Architecture Learning Curves\")\n",
    "# Save the architecture figure as a jpg\n",
    "# plt.savefig(\"hana_architectures_DiffArch_P2_1Update.jpg\")\n",
    "\n",
    "# Show figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter 2:** Epsilon Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining final architecture, as chosen from above\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "buffer_size = 50000\n",
    "batch_size = 128\n",
    "update_frequency = 1\n",
    "learning_rate = 0.001\n",
    "NUM_RUNS = 10\n",
    "# This is what we vary this time - epsilon decays\n",
    "EPSILON = 1\n",
    "epsilon_decay_range = [0.99, 0.95, 0.9, 0.8]\n",
    "min_epsilon = 0.1\n",
    "epsilon_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# RUNNING CODE FOR DQN TESTING\n",
    "# Array to store results per run\n",
    "runs_results = []\n",
    "# Make CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "for i in range(len(epsilon_decay_range)):\n",
    "    print('Epsilon: ', epsilon_decay_range[i])\n",
    "    runs_results = []\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "        # Create the main and target networks\n",
    "        policy_net = DQN(architecture_final)\n",
    "        target_net = DQN(architecture_final)\n",
    "        # Update the target with the main\n",
    "        update_target(target_net, policy_net)\n",
    "        # Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "        target_net.eval()\n",
    "        \n",
    "        # Create an SGC optimizer on the policy network parameters - CHANGE\n",
    "        #optimizer = optim.adam()\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "        # Create a memory buffer with 1 item\n",
    "        memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Number of steps done per run\n",
    "        steps_done = 0\n",
    "        # Storing all the episodes\n",
    "        episode_durations = []\n",
    "\n",
    "        # For each episode in episode_range\n",
    "        for i_episode in range(300):\n",
    "            # Reset epsilon every episode\n",
    "            EPSILON = 1\n",
    "            # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "            if (i_episode+1) % 50 == 0:\n",
    "                print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "            # Reset the environment\n",
    "            observation, info = env.reset()\n",
    "            # Turn the observation into a state\n",
    "            state = torch.tensor(observation).float()\n",
    "\n",
    "            done = False\n",
    "            terminated = False\n",
    "            t = 0\n",
    "            # While not done with episode\n",
    "            while not (done or terminated):\n",
    " \n",
    "                # Select and perform an action\n",
    "                action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "                # Take a step\n",
    "                observation, reward, done, terminated, info = env.step(action)\n",
    "                # Get the reward and action as tensors\n",
    "                reward = torch.tensor([reward])\n",
    "                action = torch.tensor([action])\n",
    "                # Define the next state as the observation but in tensor format\n",
    "                next_state = torch.tensor(observation).reshape(-1).float()\n",
    "                # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "                memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                if not len(memory.buffer) < batch_size:\n",
    "                    # Get a random sample from the buffer\n",
    "                    transitions = memory.sample(batch_size)\n",
    "                    # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "                    # sampled transition\n",
    "                    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                    # Compute the MSE loss between the target and policy\n",
    "                    mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    mse_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # If not done as in haven't reached end or terminated, append timestep\n",
    "                if done or terminated:\n",
    "                    episode_durations.append(t + 1)\n",
    "                t = t + 1\n",
    "\n",
    "                 # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION\n",
    "                if EPSILON > min_epsilon:\n",
    "                    EPSILON = EPSILON * epsilon_decay_range[i]\n",
    "                else:\n",
    "                    EPSILON = min_epsilon\n",
    "               \n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % update_frequency == 0: \n",
    "                # Update the target\n",
    "                update_target(target_net, policy_net)\n",
    "        # Append episode to results of run\n",
    "        runs_results.append(episode_durations)\n",
    "    epsilon_runs.append(runs_results)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# Save results of epsilon varying\n",
    "# import pickle\n",
    "# with open ('epsilon_1Update.pkl', 'wb') as f:\n",
    "#     pickle.dump(epsilon_runs, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Plot results of epsilon varying\n",
    "results0 = torch.tensor(epsilon_runs[0])\n",
    "means0 = results0.float().mean(0)\n",
    "stds0 = results0.float().std(0)\n",
    "\n",
    "results1 = torch.tensor(epsilon_runs[1])\n",
    "means1 = results1.float().mean(0)\n",
    "stds1 = results1.float().std(0)\n",
    "\n",
    "results2 = torch.tensor(epsilon_runs[2])\n",
    "means2 = results2.float().mean(0)\n",
    "stds2 = results2.float().std(0)\n",
    "\n",
    "results3 = torch.tensor(epsilon_runs[3])\n",
    "means3 = results3.float().mean(0)\n",
    "stds3 = results3.float().std(0)\n",
    "\n",
    "plt.plot(torch.arange(300), means0, label=\"Epsilon = 0.99\")\n",
    "plt.plot(torch.arange(300), means1, label=\"Epsilon = 0.95\")\n",
    "plt.plot(torch.arange(300), means2, label=\"Epsilon = 0.9\")\n",
    "plt.plot(torch.arange(300), means3, label=\"Epsilon = 0.8\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.fill_between(np.arange(300), means0-stds0, means0+stds0, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means1-stds1, means1+stds1, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means2-stds2, means2+stds2, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means3-stds3, means3+stds3, alpha=0.2, color='dimgray')\n",
    "\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Different Epsilon Learning Curves\")\n",
    "# plt.savefig(\"hana_architectures_DiffEpChangeInEp_1Update.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, we just want to see how fast epsilon decays as we progress through the episodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [i for i in range(300)]\n",
    "epsilonRange = [0.99, 0.95, 0.9, 0.8]\n",
    "epsilon99 = [1 * math.pow(0.99, i) for i in range(300)]\n",
    "epsilon95 = [1 * math.pow(0.95, i) for i in range(300)]\n",
    "epsilon9 = [1 * math.pow(0.9, i) for i in range(300)]\n",
    "epsilon8 = [1 * math.pow(0.8, i) for i in range(300)]\n",
    "plt.plot(numbers, epsilon99, label=\"Epsilon = 0.99\")\n",
    "plt.plot(numbers, epsilon95, label=\"Epsilon = 0.95\")\n",
    "plt.plot(numbers, epsilon9, label=\"Epsilon = 0.9\")\n",
    "plt.plot(numbers, epsilon8, label=\"Epsilon = 0.8\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Different Epsilon Decay Rates\")\n",
    "# plt.savefig(\"hana_architectures_DiffEpDecay.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter 3:** Buffer Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining final architecture, as chosen from above\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "# This is what we vary this time - buffer size\n",
    "buffer_size = [10000, 30000, 50000, 60000, 80000, 100000]\n",
    "batch_size = 128\n",
    "update_frequency = 1\n",
    "learning_rate = 0.001\n",
    "NUM_RUNS = 10\n",
    "EPSILON = 1\n",
    "epsilon_decay = 0.8\n",
    "min_epsilon = 0.1\n",
    "buffer_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_results = []\n",
    "env = gym.make('CartPole-v1')\n",
    "for i in range(len(buffer_size)):\n",
    "    print('Buffer: ', buffer_size[i])\n",
    "    runs_results = []\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "        # Create the main and target networks\n",
    "        policy_net = DQN(architecture_final)\n",
    "        target_net = DQN(architecture_final)\n",
    "        # Update the target with the main\n",
    "        update_target(target_net, policy_net)\n",
    "        # Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "        target_net.eval()\n",
    "        \n",
    "        # Create an SGC optimizer on the policy network parameters - CHANGE\n",
    "        #optimizer = optim.adam()\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "        # Create a memory buffer with 1 item\n",
    "        memory = ReplayBuffer(buffer_size[i])\n",
    "\n",
    "        # Number of steps done per run\n",
    "        steps_done = 0\n",
    "        # Storing all the episodes\n",
    "        episode_durations = []\n",
    "\n",
    "        # For each episode in episode_range\n",
    "        for i_episode in range(300):\n",
    "            # Reset epsilon\n",
    "            EPSILON = 1\n",
    "            # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "            if (i_episode+1) % 50 == 0:\n",
    "                print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "            # Reset the environment\n",
    "            observation, info = env.reset()\n",
    "            # Turn the observation into a state\n",
    "            state = torch.tensor(observation).float()\n",
    "\n",
    "            done = False\n",
    "            terminated = False\n",
    "            t = 0\n",
    "            # While not done with episode\n",
    "            while not (done or terminated):\n",
    "\n",
    "                # Select and perform an action\n",
    "                action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "                # Take a step\n",
    "                observation, reward, done, terminated, info = env.step(action)\n",
    "                # Get the reward and action as tensors\n",
    "                reward = torch.tensor([reward])\n",
    "                action = torch.tensor([action])\n",
    "                # Define the next state as the observation but in tensor format\n",
    "                next_state = torch.tensor(observation).reshape(-1).float()\n",
    "                # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "                memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                if not len(memory.buffer) < batch_size:\n",
    "                    # Get a random sample from the buffer\n",
    "                    transitions = memory.sample(batch_size)\n",
    "                    # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "                    # sampled transition\n",
    "                    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                    # Compute the MSE loss between the target and policy\n",
    "                    mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    mse_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # If not done as in haven't reached end or terminated, append timestep\n",
    "                if done or terminated:\n",
    "                    episode_durations.append(t + 1)\n",
    "                t = t + 1\n",
    "\n",
    "                # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION\n",
    "                if EPSILON > min_epsilon:\n",
    "                    EPSILON = EPSILON * epsilon_decay\n",
    "                else:\n",
    "                    EPSILON = min_epsilon\n",
    "                \n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % update_frequency == 0: \n",
    "                # Update the target\n",
    "                update_target(target_net, policy_net)\n",
    "        # Append episode to results of run\n",
    "        runs_results.append(episode_durations)\n",
    "    buffer_runs.append(runs_results)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## Save results of each buffer run\n",
    "# import pickle\n",
    "# with open ('buffer_1Update.pkl', 'wb') as f:\n",
    "#     pickle.dump(buffer_runs, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results0 = torch.tensor(buffer_runs[0])\n",
    "means0 = results0.float().mean(0)\n",
    "stds0 = results0.float().std(0)\n",
    "\n",
    "results1 = torch.tensor(buffer_runs[1])\n",
    "means1 = results1.float().mean(0)\n",
    "stds1 = results1.float().std(0)\n",
    "\n",
    "results2 = torch.tensor(buffer_runs[2])\n",
    "means2 = results2.float().mean(0)\n",
    "stds2 = results2.float().std(0)\n",
    "\n",
    "results3 = torch.tensor(buffer_runs[3])\n",
    "means3 = results3.float().mean(0)\n",
    "stds3 = results3.float().std(0)\n",
    "\n",
    "results4 = torch.tensor(buffer_runs[4])\n",
    "means4 = results4.float().mean(0)\n",
    "stds4 = results4.float().std(0)\n",
    "\n",
    "results5 = torch.tensor(buffer_runs[5])\n",
    "means5 = results5.float().mean(0)\n",
    "stds5 = results5.float().std(0)\n",
    "\n",
    "plt.plot(torch.arange(300), means0, label=\"Buffer size = 10000\")\n",
    "plt.plot(torch.arange(300), means1, label=\"Buffer size = 30000\")\n",
    "plt.plot(torch.arange(300), means2, label=\"Buffer size = 50000\")\n",
    "plt.plot(torch.arange(300), means3, label=\"Buffer size = 60000\")\n",
    "plt.plot(torch.arange(300), means4, label=\"Buffer size = 80000\")\n",
    "plt.plot(torch.arange(300), means5, label=\"Buffer size = 100000\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.fill_between(np.arange(300), means0-stds0, means0+stds0, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means1-stds1, means1+stds1, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means2-stds2, means2+stds2, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means3-stds3, means3+stds3, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means4-stds4, means4+stds4, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means5-stds5, means5+stds5, alpha=0.2, color='dimgray')\n",
    "\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Different Buffer Size Learning Curves\")\n",
    "# plt.savefig(\"hana_architectures_DiffBuffer_1Update.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter 4:** Update Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining final architecture, as chosen from above\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "buffer_size = 60000\n",
    "batch_size = 128\n",
    "# This is what we vary this time - update frequency\n",
    "update_frequency = [1, 10, 50, 100, 200]\n",
    "learning_rate = 0.001\n",
    "NUM_RUNS = 10\n",
    "EPSILON = 1\n",
    "epsilon_decay = 0.8\n",
    "min_epsilon = 0.1\n",
    "update_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_results = []\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "for i in range(len(update_frequency)):\n",
    "    print('Update Freq: ', update_frequency[i])\n",
    "    runs_results = []\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "        # Create the main and target networks\n",
    "        policy_net = DQN(architecture_final)\n",
    "        target_net = DQN(architecture_final)\n",
    "        # Update the target with the main\n",
    "        update_target(target_net, policy_net)\n",
    "        # Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "        target_net.eval()\n",
    "        \n",
    "        # Create an SGC optimizer on the policy network parameters - CHANGE\n",
    "        #optimizer = optim.adam()\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "        # Create a memory buffer with 1 item\n",
    "        memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Number of steps done per run\n",
    "        steps_done = 0\n",
    "        # Storing all the episodes\n",
    "        episode_durations = []\n",
    "\n",
    "        # For each episode in episode_range\n",
    "        for i_episode in range(300):\n",
    "            # Reset epsilon\n",
    "            EPSILON = 1\n",
    "            # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "            if (i_episode+1) % 50 == 0:\n",
    "                print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "            # Reset the environment\n",
    "            observation, info = env.reset()\n",
    "            # Turn the observation into a state\n",
    "            state = torch.tensor(observation).float()\n",
    "\n",
    "            done = False\n",
    "            terminated = False\n",
    "            t = 0\n",
    "            # While not done with episode\n",
    "            while not (done or terminated):\n",
    "\n",
    "                # Select and perform an action\n",
    "                action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "                # Take a step\n",
    "                observation, reward, done, terminated, info = env.step(action)\n",
    "                # Get the reward and action as tensors\n",
    "                reward = torch.tensor([reward])\n",
    "                action = torch.tensor([action])\n",
    "                # Define the next state as the observation but in tensor format\n",
    "                next_state = torch.tensor(observation).reshape(-1).float()\n",
    "                # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "                memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                if not len(memory.buffer) < batch_size:\n",
    "                    # Get a random sample from the buffer\n",
    "                    transitions = memory.sample(batch_size)\n",
    "                    # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "                    # sampled transition\n",
    "                    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                    # Compute the MSE loss between the target and policy\n",
    "                    mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    mse_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # If not done as in haven't reached end or terminated, append timestep\n",
    "                if done or terminated:\n",
    "                    episode_durations.append(t + 1)\n",
    "                t = t + 1\n",
    "\n",
    "                # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION\n",
    "                if EPSILON > min_epsilon:\n",
    "                    EPSILON = EPSILON * epsilon_decay\n",
    "                else:\n",
    "                    EPSILON = min_epsilon\n",
    "        \n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % update_frequency[i] == 0: \n",
    "                # Update the target\n",
    "                update_target(target_net, policy_net)\n",
    "        # Append episode to results of run\n",
    "        runs_results.append(episode_durations)\n",
    "    update_runs.append(runs_results)\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######### Save results of different update frequencies\n",
    "# import pickle\n",
    "# with open ('updateFreq.pkl', 'wb') as f:\n",
    "#     pickle.dump(update_runs, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Plot update frequency results\n",
    "results0 = torch.tensor(update_runs[0])\n",
    "means0 = results0.float().mean(0)\n",
    "stds0 = results0.float().std(0)\n",
    "\n",
    "results1 = torch.tensor(update_runs[1])\n",
    "means1 = results1.float().mean(0)\n",
    "stds1 = results1.float().std(0)\n",
    "\n",
    "results2 = torch.tensor(update_runs[2])\n",
    "means2 = results2.float().mean(0)\n",
    "stds2 = results2.float().std(0)\n",
    "\n",
    "results3 = torch.tensor(update_runs[3])\n",
    "means3 = results3.float().mean(0)\n",
    "stds3 = results3.float().std(0)\n",
    "\n",
    "results4 = torch.tensor(update_runs[4])\n",
    "means4 = results4.float().mean(0)\n",
    "stds4 = results4.float().std(0)\n",
    "\n",
    "plt.plot(torch.arange(300), means0, label=\"Update Freq = 1\")\n",
    "plt.plot(torch.arange(300), means1, label=\"Update Freq = 10\")\n",
    "plt.plot(torch.arange(300), means2, label=\"Update Freq = 50\")\n",
    "plt.plot(torch.arange(300), means3, label=\"Update Freq = 100\")\n",
    "plt.plot(torch.arange(300), means4, label=\"Update Freq = 200\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.fill_between(np.arange(300), means0-stds0, means0+stds0, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means1-stds1, means1+stds1, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means2-stds2, means2+stds2, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means3-stds3, means3+stds3, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means4-stds4, means4+stds4, alpha=0.2, color='dimgray')\n",
    "\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Different Update Frequency Learning Curves\")\n",
    "# plt.savefig(\"hana_architectures_DiffUpdate_1Update.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter 5:** Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining final architecture, as chosen from above\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "buffer_size = 60000\n",
    "batch_size = 128\n",
    "update_frequency = 1\n",
    "# This is what we vary this time - learning rates\n",
    "learning_rate = [0.001, 0.0025, 0.01, 0.1]\n",
    "NUM_RUNS = 10\n",
    "EPSILON = 1\n",
    "epsilon_decay = 0.8\n",
    "min_epsilon = 0.1\n",
    "learning_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_results = []\n",
    "env = gym.make('CartPole-v1')\n",
    "for i in range(len(learning_rate)):\n",
    "    print('Learning Rate: ', learning_rate[i])\n",
    "    runs_results = []\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "        # Create the main and target networks\n",
    "        policy_net = DQN(architecture_final)\n",
    "        target_net = DQN(architecture_final)\n",
    "        # Update the target with the main\n",
    "        update_target(target_net, policy_net)\n",
    "        # Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "        target_net.eval()\n",
    "        \n",
    "        # Create an SGC optimizer on the policy network parameters - CHANGE\n",
    "        #optimizer = optim.adam()\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate[i])\n",
    "        # Create a memory buffer with 1 item\n",
    "        memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Number of steps done per run\n",
    "        steps_done = 0\n",
    "        # Storing all the episodes\n",
    "        episode_durations = []\n",
    "\n",
    "        # For each episode in episode_range\n",
    "        for i_episode in range(300):\n",
    "            # Reset epsilon\n",
    "            EPSILON = 1\n",
    "            # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "            if (i_episode+1) % 50 == 0:\n",
    "                print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "            # Reset the environment\n",
    "            observation, info = env.reset()\n",
    "            # Turn the observation into a state\n",
    "            state = torch.tensor(observation).float()\n",
    "\n",
    "            done = False\n",
    "            terminated = False\n",
    "            t = 0\n",
    "            # While not done with episode\n",
    "            while not (done or terminated):\n",
    "\n",
    "                # Select and perform an action\n",
    "                action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "                # Take a step\n",
    "                observation, reward, done, terminated, info = env.step(action)\n",
    "                # Get the reward and action as tensors\n",
    "                reward = torch.tensor([reward])\n",
    "                action = torch.tensor([action])\n",
    "                # Define the next state as the observation but in tensor format\n",
    "                next_state = torch.tensor(observation).reshape(-1).float()\n",
    "                # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "                memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                if not len(memory.buffer) < batch_size:\n",
    "                    # Get a random sample from the buffer\n",
    "                    transitions = memory.sample(batch_size)\n",
    "                    # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "                    # sampled transition\n",
    "                    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                    # Compute the MSE loss between the target and policy\n",
    "                    mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    mse_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # If not done as in haven't reached end or terminated, append timestep\n",
    "                if done or terminated:\n",
    "                    episode_durations.append(t + 1)\n",
    "                t = t + 1\n",
    "\n",
    "                # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION\n",
    "                if EPSILON > min_epsilon:\n",
    "                    EPSILON = EPSILON * epsilon_decay\n",
    "                else:\n",
    "                    EPSILON = min_epsilon\n",
    "\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % update_frequency == 0: \n",
    "                # Update the target\n",
    "                update_target(target_net, policy_net)\n",
    "        # Append episode to results of run\n",
    "        runs_results.append(episode_durations)\n",
    "    learning_runs.append(runs_results)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## Save results\n",
    "# import pickle\n",
    "# with open ('learingRate.pkl', 'wb') as f:\n",
    "#     pickle.dump(learning_runs, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Plot results\n",
    "results0 = torch.tensor(learning_runs[0])\n",
    "means0 = results0.float().mean(0)\n",
    "stds0 = results0.float().std(0)\n",
    "\n",
    "results1 = torch.tensor(learning_runs[1])\n",
    "means1 = results1.float().mean(0)\n",
    "stds1 = results1.float().std(0)\n",
    "\n",
    "results2 = torch.tensor(learning_runs[2])\n",
    "means2 = results2.float().mean(0)\n",
    "stds2 = results2.float().std(0)\n",
    "\n",
    "results3 = torch.tensor(learning_runs[3])\n",
    "means3 = results3.float().mean(0)\n",
    "stds3 = results3.float().std(0)\n",
    "\n",
    "plt.plot(torch.arange(300), means0, label=\"LR = 0.001\")\n",
    "plt.plot(torch.arange(300), means1, label=\"LR = 0.0025\")\n",
    "plt.plot(torch.arange(300), means2, label=\"LR = 0.01\")\n",
    "plt.plot(torch.arange(300), means3, label=\"LR = 0.1\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.fill_between(np.arange(300), means0-stds0, means0+stds0, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means1-stds1, means1+stds1, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means2-stds2, means2+stds2, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means3-stds3, means3+stds3, alpha=0.2, color='dimgray')\n",
    "\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Different Learning Rate Learning Curves\")\n",
    "# plt.savefig(\"hana_architectures_DiffLR_1Update.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter 6:** Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining final architecture, as chosen from above\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "buffer_size = 60000\n",
    "# This is what we vary this time - batch size\n",
    "batch_size = [32, 64, 128, 256, 512]\n",
    "update_frequency = 1\n",
    "learning_rate = 0.0025\n",
    "NUM_RUNS = 10\n",
    "EPSILON = 1\n",
    "epsilon_decay = 0.8\n",
    "min_epsilon = 0.1\n",
    "batch_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_results = []\n",
    "env = gym.make('CartPole-v1')\n",
    "for i in range(len(batch_size)):\n",
    "    print('Batch sizes: ', batch_size[i])\n",
    "    runs_results = []\n",
    "    for run in range(NUM_RUNS):\n",
    "        print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "        # Create the main and target networks\n",
    "        policy_net = DQN(architecture_final)\n",
    "        target_net = DQN(architecture_final)\n",
    "        # Update the target with the main\n",
    "        update_target(target_net, policy_net)\n",
    "        # Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "        target_net.eval()\n",
    "        \n",
    "        # Create an SGC optimizer on the policy network parameters - CHANGE\n",
    "        #optimizer = optim.adam()\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "        # Create a memory buffer with 1 item\n",
    "        memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Number of steps done per run\n",
    "        steps_done = 0\n",
    "        # Storing all the episodes\n",
    "        episode_durations = []\n",
    "\n",
    "        # For each episode in episode_range\n",
    "        for i_episode in range(300):\n",
    "            # Reset epsilon\n",
    "            EPSILON = 1\n",
    "            # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "            if (i_episode+1) % 50 == 0:\n",
    "                print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "            # Reset the environment\n",
    "            observation, info = env.reset()\n",
    "            # Turn the observation into a state\n",
    "            state = torch.tensor(observation).float()\n",
    "\n",
    "            done = False\n",
    "            terminated = False\n",
    "            t = 0\n",
    "            # While not done with episode\n",
    "            while not (done or terminated):\n",
    "\n",
    "                # Select and perform an action\n",
    "                action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "                # Take a step\n",
    "                observation, reward, done, terminated, info = env.step(action)\n",
    "                # Get the reward and action as tensors\n",
    "                reward = torch.tensor([reward])\n",
    "                action = torch.tensor([action])\n",
    "                # Define the next state as the observation but in tensor format\n",
    "                next_state = torch.tensor(observation).reshape(-1).float()\n",
    "                # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "                memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                if not len(memory.buffer) < batch_size[i]:\n",
    "                    # Get a random sample from the buffer\n",
    "                    transitions = memory.sample(batch_size[i])\n",
    "                    # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "                    # sampled transition\n",
    "                    state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                    # Compute the MSE loss between the target and policy\n",
    "                    mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    mse_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # If not done as in haven't reached end or terminated, append timestep\n",
    "                if done or terminated:\n",
    "                    episode_durations.append(t + 1)\n",
    "                t = t + 1\n",
    "\n",
    "                # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION\n",
    "                if EPSILON > min_epsilon:\n",
    "                    EPSILON = EPSILON * epsilon_decay\n",
    "                else:\n",
    "                    EPSILON = min_epsilon\n",
    "\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % update_frequency == 0: \n",
    "                # Update the target\n",
    "                update_target(target_net, policy_net)\n",
    "        # Append episode to results of run\n",
    "        runs_results.append(episode_durations)\n",
    "    batch_runs.append(runs_results)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######### Save results of batch size changing\n",
    "# import pickle\n",
    "# with open ('batchSizes.pkl', 'wb') as f:\n",
    "#     pickle.dump(batch_runs, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Plot results of batch\n",
    "results0 = torch.tensor(batch_runs[0])\n",
    "means0 = results0.float().mean(0)\n",
    "stds0 = results0.float().std(0)\n",
    "\n",
    "results1 = torch.tensor(batch_runs[1])\n",
    "means1 = results1.float().mean(0)\n",
    "stds1 = results1.float().std(0)\n",
    "\n",
    "results2 = torch.tensor(batch_runs[2])\n",
    "means2 = results2.float().mean(0)\n",
    "stds2 = results2.float().std(0)\n",
    "\n",
    "results3 = torch.tensor(batch_runs[3])\n",
    "means3 = results3.float().mean(0)\n",
    "stds3 = results3.float().std(0)\n",
    "\n",
    "results4 = torch.tensor(batch_runs[4])\n",
    "means4 = results4.float().mean(0)\n",
    "stds4 = results4.float().std(0)\n",
    "\n",
    "plt.plot(torch.arange(300), means0, label=\"Batch Size = 32\")\n",
    "plt.plot(torch.arange(300), means1, label=\"Batch Size = 64\")\n",
    "plt.plot(torch.arange(300), means2, label=\"Batch Size = 128\")\n",
    "plt.plot(torch.arange(300), means3, label=\"Batch Size = 256\")\n",
    "plt.plot(torch.arange(300), means4, label=\"Batch Size = 512\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.fill_between(np.arange(300), means0-stds0, means0+stds0, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means1-stds1, means1+stds1, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means2-stds2, means2+stds2, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means3-stds3, means3+stds3, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means4-stds4, means4+stds4, alpha=0.2, color='dimgray')\n",
    "\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Different Batch Size Learning Curves\")\n",
    "# plt.savefig(\"hana_architectures_DiffBatch_1Update.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1.3: Final Parameter Learning Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0025\n",
    "batch_size = 128\n",
    "buffer_size = 60000\n",
    "update_frequency = 1\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "epsilon_decay = 0.8\n",
    "NUM_RUNS = 10\n",
    "EPSILON = 1\n",
    "min_epsilon = 0.1\n",
    "runs_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "runs_results = []\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "    # Create the main and target networks\n",
    "    policy_net = DQN(architecture_final)\n",
    "    target_net = DQN(architecture_final)\n",
    "    # Update the target with the main\n",
    "    update_target(target_net, policy_net)\n",
    "    # Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "    target_net.eval()\n",
    "    \n",
    "    # Create an SGC optimizer on the policy network parameters - CHANGE\n",
    "    #optimizer = optim.adam()\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    # Create a memory buffer with 1 item\n",
    "    memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "    # Number of steps done per run\n",
    "    steps_done = 0\n",
    "    # Storing all the episodes\n",
    "    episode_durations = []\n",
    "\n",
    "    # For each episode in episode_range\n",
    "    for i_episode in range(300):\n",
    "        # Reset epsilon\n",
    "        EPSILON = 1\n",
    "        # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "        if (i_episode+1) % 50 == 0:\n",
    "            print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "        # Reset the environment\n",
    "        observation, info = env.reset()\n",
    "        # Turn the observation into a state\n",
    "        state = torch.tensor(observation).float()\n",
    "\n",
    "        done = False\n",
    "        terminated = False\n",
    "        t = 0\n",
    "        # While not done with episode\n",
    "        while not (done or terminated):\n",
    "            \n",
    "            # Select and perform an action\n",
    "            action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "            # Take a step\n",
    "            observation, reward, done, terminated, info = env.step(action)\n",
    "            # Get the reward and action as tensors\n",
    "            reward = torch.tensor([reward])\n",
    "            action = torch.tensor([action])\n",
    "            # Define the next state as the observation but in tensor format\n",
    "            next_state = torch.tensor(observation).reshape(-1).float()\n",
    "            # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "            memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if not len(memory.buffer) < batch_size:\n",
    "                # Get a random sample from the buffer\n",
    "                transitions = memory.sample(batch_size)\n",
    "                # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "                # sampled transition\n",
    "                state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                # Compute the MSE loss between the target and policy\n",
    "                mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                mse_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # If not done as in haven't reached end or terminated, append timestep\n",
    "            if done or terminated:\n",
    "                episode_durations.append(t + 1)\n",
    "            t = t + 1\n",
    "\n",
    "            # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION\n",
    "            if EPSILON > min_epsilon:\n",
    "                EPSILON = EPSILON * epsilon_decay\n",
    "            else:\n",
    "                EPSILON = min_epsilon\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % update_frequency == 0: \n",
    "            # Update the target\n",
    "            update_target(target_net, policy_net)\n",
    "    # Append episode to results of run\n",
    "    runs_results.append(episode_durations)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## Save results in a pickle file\n",
    "# import pickle\n",
    "# with open ('finalRun2.pkl', 'wb') as f:\n",
    "#     pickle.dump(runs_results, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## Open pickle file of results\n",
    "# import pickle\n",
    "# with open('finalRun2.pkl', 'rb') as f:\n",
    "#     final_run = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Plot the final run results\n",
    "results = torch.tensor(runs_results)\n",
    "means = results.float().mean(0)\n",
    "stds = results.float().std(0)\n",
    "\n",
    "plt.plot(torch.arange(300), means, label=\"Mean of Return\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.fill_between(np.arange(300), means-stds, means+stds, alpha=0.2, color='dimgray')\n",
    "\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Final Model Learning Curve\")\n",
    "# plt.savefig(\"hana_architectures_finalModel2.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2: Visualizing DQN**\n",
    "In this segment, we observe what the agent learns in terms of its policy for how to navigate the problem space (what action to take and when) as a function of the pole's angle and angular velocity. To do this, we do *one* training run of the DQN policy and target networks, then use the provided functions to print out what the results of training are for different cart velocities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the Network once more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0025\n",
    "batch_size = 128\n",
    "buffer_size = 60000\n",
    "update_frequency = 1\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "epsilon_decay = 0.8\n",
    "EPSILON = 1\n",
    "NUM_RUNS = 10\n",
    "min_epsilon = 0.1\n",
    "runs_results = []\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# Create the main and target networks\n",
    "policy_net = DQN(architecture_final)\n",
    "target_net = DQN(architecture_final)\n",
    "# Update the target with the main\n",
    "update_target(target_net, policy_net)\n",
    "# Evaluate the target network, which is the one we want to optimize at the end of the day\n",
    "target_net.eval()\n",
    "\n",
    "# Create an SGC optimizer on the policy network parameters - CHANGE\n",
    "#optimizer = optim.adam()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "# Create a memory buffer with 1 item\n",
    "memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "# Number of steps done per run\n",
    "steps_done = 0\n",
    "# Storing all the episodes\n",
    "episode_durations = []\n",
    "\n",
    "# For each episode in episode_range\n",
    "for i_episode in range(300):\n",
    "    # Reset epsilon\n",
    "    EPSILON = 1\n",
    "    # Every 50 episodes, we say that we're at episode something out of episode_range\n",
    "    if (i_episode+1) % 50 == 0:\n",
    "        print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "    # Reset the environment\n",
    "    observation, info = env.reset()\n",
    "    # Turn the observation into a state\n",
    "    state = torch.tensor(observation).float()\n",
    "\n",
    "    done = False\n",
    "    terminated = False\n",
    "    t = 0\n",
    "    # While not done with episode\n",
    "    while not (done or terminated):\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "        # Take a step\n",
    "        observation, reward, done, terminated, info = env.step(action)\n",
    "        # Get the reward and action as tensors\n",
    "        reward = torch.tensor([reward])\n",
    "        action = torch.tensor([action])\n",
    "        # Define the next state as the observation but in tensor format\n",
    "        next_state = torch.tensor(observation).reshape(-1).float()\n",
    "        # Push the state, next state, action and etc to memory, and then whether or not it's done\n",
    "        memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        if not len(memory.buffer) < batch_size:\n",
    "            # Get a random sample from the buffer\n",
    "            transitions = memory.sample(batch_size)\n",
    "            # torch.stack() concatenates a sequence of tensors along a new dimension. Here, we concatenate all the information of the\n",
    "            # sampled transition\n",
    "            state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "            # Compute the MSE loss between the target and policy\n",
    "            mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            mse_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # If not done as in haven't reached end or terminated, append timestep\n",
    "        if done or terminated:\n",
    "            episode_durations.append(t + 1)\n",
    "        t = t + 1\n",
    "\n",
    "        # SET MINIMUM EPSILON SO IT ALWAYS HAS EXPLORATION\n",
    "        if EPSILON > min_epsilon:\n",
    "            EPSILON = EPSILON * epsilon_decay\n",
    "        else:\n",
    "            EPSILON = min_epsilon\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % update_frequency == 0: \n",
    "        # Update the target\n",
    "        update_target(target_net, policy_net)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## Saving results of one run\n",
    "# import pickle\n",
    "# with open ('oneRun.pkl', 'wb') as f:\n",
    "#     pickle.dump(episode_durations, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## Saving policy of one run\n",
    "# with open ('oneRunPolicy.pkl', 'wb') as f:\n",
    "#     pickle.dump(policy_net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## Loading up the policy of one run\n",
    "# with open('oneRunPolicy.pkl', 'rb') as f:\n",
    "#     policy_net = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 2.1: Visualizing Slices of Greedy Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the greedy Q-values for a stationary cart in the middle of the track\n",
    "# 2D plot showing policy as a function of pole angle and angular velocity (omega)\n",
    "\n",
    "# This plots the policy and Q values according to the network currently\n",
    "# stored in the variable \"policy_net\"\n",
    "\n",
    "# All visualisations provided here are placeholders and you can modify these plots\n",
    "\n",
    "# Make sure to include appropriate labels and/or legends when presenting your plot\n",
    "\n",
    "q = False    # whether q values or greedy policy is visualised\n",
    "\n",
    "angle_range = .2095 # you may modify this range\n",
    "omega_range = 5     # you may modify this range\n",
    "\n",
    "angle_samples = 100\n",
    "omega_samples = 100\n",
    "angles = torch.linspace(angle_range, -angle_range, angle_samples)\n",
    "omegas = torch.linspace(-omega_range, omega_range, omega_samples)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "velocities = [0, 0.5, 1, 2]\n",
    "plt.suptitle(\"Greedy Policy Visualization for Different Velocities, Yellow = Right, Blue = Left\")\n",
    "\n",
    "for iter, velo in enumerate(velocities):\n",
    "    # Create the subplot axes for each iteration in the velocities array\n",
    "    ax = plt.subplot(3, 2, iter + 1)\n",
    "    # Create the q-value and policy tensor as empty tensors of size (angles, angular velocities)\n",
    "    greedy_q_array = torch.zeros((angle_samples, omega_samples))\n",
    "    policy_array = torch.zeros((angle_samples, omega_samples))\n",
    "    # For each angle\n",
    "    for i, angle in enumerate(angles):\n",
    "        # For each angular velocity\n",
    "        for j, omega in enumerate(omegas):\n",
    "            # Define the state as a tensor\n",
    "            state = torch.tensor([0., velo, angle, omega])\n",
    "            with torch.no_grad():\n",
    "                # Get the q values, the greedy action as the index of the maximum of the q values, then populate arrays\n",
    "                q_vals = policy_net(state)\n",
    "                greedy_action = q_vals.argmax()\n",
    "                greedy_q_array[i, j] = q_vals[greedy_action]\n",
    "                policy_array[i, j] = greedy_action\n",
    "    # If Q value is what's being visualized, visualize with color bar\n",
    "    if q:\n",
    "        qValueFig = ax.contourf(angles, omegas, greedy_q_array.T, cmap='cividis', levels=100)\n",
    "        fig.colorbar(qValueFig)\n",
    "    # If greedy policy is what's being visualized, visualize the contour without colorbar\n",
    "    else:\n",
    "        policyFig = ax.contourf(angles, omegas, policy_array.T, cmap='cividis')\n",
    "    \n",
    "    ax.set_title(\"Cart velocity of {}\".format(velo))\n",
    "    ax.set_xlabel(\"pole angle\")\n",
    "    ax.set_ylabel(\"pole angular velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 2.2: Visualizing Slices of Q Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the greedy Q-values for a stationary cart in the middle of the track\n",
    "# 2D plot showing policy as a function of pole angle and angular velocity (omega)\n",
    "\n",
    "# This plots the policy and Q values according to the network currently\n",
    "# stored in the variable \"policy_net\"\n",
    "\n",
    "# All visualisations provided here are placeholders and you can modify these plots\n",
    "\n",
    "# Make sure to include appropriate labels and/or legends when presenting your plot\n",
    "\n",
    "q = True    # whether q values or greedy policy is visualised\n",
    "\n",
    "angle_range = .2095 # you may modify this range\n",
    "omega_range = 4     # you may modify this range\n",
    "\n",
    "angle_samples = 100\n",
    "omega_samples = 100\n",
    "angles = torch.linspace(angle_range, -angle_range, angle_samples)\n",
    "omegas = torch.linspace(-omega_range, omega_range, omega_samples)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "velocities = [0, 0.5, 1, 2]\n",
    "plt.suptitle(\"Greedy Q-Value Visualization for Different Velocities, Yellow = Right, Blue = Left\")\n",
    "\n",
    "for iter, velo in enumerate(velocities):\n",
    "    # Create the subplot axes for each iteration in the velocities array\n",
    "    ax = plt.subplot(3, 2, iter + 1)\n",
    "    # Create the q-value and policy tensor as empty tensors of size (angles, angular velocities)\n",
    "    greedy_q_array = torch.zeros((angle_samples, omega_samples))\n",
    "    policy_array = torch.zeros((angle_samples, omega_samples))\n",
    "    # For each angle\n",
    "    for i, angle in enumerate(angles):\n",
    "        # For each angular velocity\n",
    "        for j, omega in enumerate(omegas):\n",
    "            # Define the state as a tensor\n",
    "            state = torch.tensor([0., velo, angle, omega])\n",
    "            with torch.no_grad():\n",
    "                # Get the q values, the greedy action as the index of the maximum of the q values, then populate arrays\n",
    "                q_vals = policy_net(state)\n",
    "                greedy_action = q_vals.argmax()\n",
    "                greedy_q_array[i, j] = q_vals[greedy_action]\n",
    "                policy_array[i, j] = greedy_action\n",
    "    # If Q value is what's being visualized, visualize with color bar\n",
    "    if q:\n",
    "        qValueFig = ax.contourf(angles, omegas, greedy_q_array.T, cmap='cividis', levels=100)\n",
    "        fig.colorbar(qValueFig)\n",
    "    # If greedy policy is what's being visualized, visualize the contour without colorbar\n",
    "    else:\n",
    "        policyFig = ax.contourf(angles, omegas, policy_array.T, cmap='cividis')\n",
    "    \n",
    "    ax.set_title(\"Cart velocity of {}\".format(velo))\n",
    "    ax.set_xlabel(\"pole angle\")\n",
    "    ax.set_ylabel(\"pole angular velocity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3: Transforming DQN into DDQN**\n",
    "In the final segment, we convert the DQN into a DDQN. This can be done by changing the loss function appropriately. Ultimately, what we want to do is to be able to decouple the process of picking an action from evaluating or estimating the value of that action, in order to prevent overestimation inherent to DQNs.\n",
    "(Note that for my specific code, I compare DDQN with DQN using the finalRun2.pkl file, which contains the list of run results from the 10-run learning curve used in Part 1.3. Thus, to properly run the code, please make sure to run Part 1.3, and change the variable in the last plotting function to <i>run_results</i> rather than <i>dqn_runs</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0025\n",
    "batch_size = 128\n",
    "buffer_size = 60000\n",
    "update_frequency = 1\n",
    "architecture_final = [4, 64, 32, 2]\n",
    "epsilon_decay = 0.8\n",
    "EPSILON = 1\n",
    "NUM_RUNS = 10\n",
    "min_epsilon = 0.1\n",
    "# New array to store DDQN run results\n",
    "ddqn_runs = []  \n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# For each run\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "    # Create the networks\n",
    "    policy_net = DQN(architecture_final)\n",
    "    target_net = DQN(architecture_final)\n",
    "    update_target(target_net, policy_net)\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "    steps_done = 0\n",
    "\n",
    "    episode_durations = []\n",
    "    \n",
    "    for i_episode in range(300):\n",
    "        \n",
    "        if (i_episode+1) % 50 == 0:\n",
    "            print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "        # Reset epsilon at each episode, and observe the state from reset\n",
    "        EPSILON = 1\n",
    "        observation, info = env.reset()\n",
    "        state = torch.tensor(observation).float()\n",
    "\n",
    "        done = False\n",
    "        terminated = False\n",
    "        t = 0    \n",
    "\n",
    "        while not (done or terminated):\n",
    "            # Select and perform an action from the policy network\n",
    "            action = epsilon_greedy(EPSILON, policy_net, state)\n",
    "            # Proceed as before, in which you obtain the observation, states, etc\n",
    "            observation, reward, done, terminated, info = env.step(action)\n",
    "            reward = torch.tensor([reward])\n",
    "            action = torch.tensor([action])\n",
    "            next_state = torch.tensor(observation).reshape(-1).float()\n",
    "            # Push all the information into memory\n",
    "            memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            # Set boolean that we are finding ddqn\n",
    "            ddqn = True\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if not len(memory.buffer) < batch_size:\n",
    "                transitions = memory.sample(batch_size)\n",
    "                state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                # Compute loss, this time with the ddqn flag\n",
    "                mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones, ddqn)\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                mse_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            if done or terminated:\n",
    "                episode_durations.append(t + 1)\n",
    "            t = t + 1\n",
    "            \n",
    "            # Update epsilon at the end of each step\n",
    "            if EPSILON > min_epsilon:\n",
    "                EPSILON = EPSILON * epsilon_decay\n",
    "                \n",
    "            else:\n",
    "                EPSILON = min_epsilon\n",
    "                \n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % update_frequency == 0: \n",
    "            update_target(target_net, policy_net)\n",
    "\n",
    "    # Append the current episode's results to the DDQN runs\n",
    "    ddqn_runs.append(episode_durations)\n",
    "\n",
    "print('Completed runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######## Save results of the DDQN run\n",
    "# import pickle\n",
    "# with open ('ddqn2.pkl', 'wb') as f:\n",
    "#     pickle.dump(ddqn_runs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### Load results of DQN final runs\n",
    "# import pickle\n",
    "# with open('finalRun2.pkl', 'rb') as f:\n",
    "#     dqn_runs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Plotting results of DQN vs DDQN\n",
    "ddqn_results = torch.tensor(ddqn_runs)\n",
    "ddqn_mean = ddqn_results.float().mean(0)\n",
    "ddqn_std = ddqn_results.float().std(0)\n",
    "\n",
    "results_DQN = torch.tensor(dqn_runs)\n",
    "means_DQN = results_DQN.float().mean(0)\n",
    "stds_DQN = results_DQN.float().std(0)\n",
    "\n",
    "# means_avg = results2.float().mean(0)\n",
    "\n",
    "plt.plot(torch.arange(300), ddqn_mean, label=\"DDQN\")\n",
    "plt.plot(torch.arange(300), means_DQN, label=\"DQN\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.fill_between(np.arange(300), ddqn_mean, ddqn_mean+ddqn_std, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), ddqn_mean, ddqn_mean-ddqn_std, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means_DQN, means_DQN+stds_DQN, alpha=0.2, color='dimgray')\n",
    "plt.fill_between(np.arange(300), means_DQN, means_DQN-stds_DQN, alpha=0.2, color='dimgray')\n",
    "plt.axhline(y=100, linestyle='--', color='r')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.title(\"Learning curves for the DQN and DDQN\")\n",
    "# plt.savefig(\"ddqn_lc.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('python39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4aa17b5241b599c9a8351bf8f0bfbba550ff4751b84201d580961979028a2271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
